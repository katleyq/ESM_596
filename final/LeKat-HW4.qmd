---
title: "PSTAT 231 - Homework 4"
author: "Kat Le"
format:
  html:
    embed-resources: true 
    toc_float: true
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Tree-Based Models

For this assignment, we will be working with the file `"pokemon.csv"`, found in `/data`. The file is from Kaggle: <https://www.kaggle.com/abcsds/pokemon>.

The [Pokémon](https://www.pokemon.com/us/) franchise encompasses video games, TV shows, movies, books, and a card game. This data set was drawn from the video game series and contains statistics about 721 Pokémon, or "pocket monsters." In Pokémon games, the user plays as a trainer who collects, trades, and battles Pokémon to (a) collect all the Pokémon and (b) become the champion Pokémon trainer.

Each Pokémon has a [primary type](https://bulbapedia.bulbagarden.net/wiki/Type) (some even have secondary types). Based on their type, a Pokémon is strong against some types, and vulnerable to others. (Think rock, paper, scissors.) A Fire-type Pokémon, for example, is vulnerable to Water-type Pokémon, but strong against Grass-type.

The goal of this assignment is to build a statistical learning model that can predict the **primary type** of a Pokémon based on its generation, legendary status, and six battle statistics.

**Note: Fitting ensemble tree-based models can take a little while to run. Consider running your models outside of the .Rmd, storing the results, and loading them in your .Rmd to minimize time to knit.**

### Exercise 1: Preprocessing the data

Read in the data and set things up:

a.  Install and load the `janitor` package. Use its `clean_names()` function on the Pokémon data.

```{r message=FALSE}
# Set seed for reproducibility
set.seed(123)

# Load relevent libraries
library(here)
library(janitor)
library(tidyverse)
library(tidymodels)
library(gt)
library(glmnet)
library(xgboost)
library(corrplot)
library(rpart.plot)
library(vip)

# Load pokemon data
pokemon <- read_csv(here("homework-4/data/Pokemon.csv")) %>% 
  clean_names()
```

a.  Filter out the rarer Pokémon types: Using the entire data set, create a bar chart of the outcome variable, `type_1`.

```{r}
# Create bar chart of type_1 count within pokemon dataset. Order by Count
pokemon %>%
  count(type_1) %>%  # Count occurrences of each type_1
  mutate(type_1 = fct_reorder(type_1, n, .desc = FALSE)) %>%
  ggplot(aes(x = type_1, y = n)) +  
  geom_bar(stat = "identity", fill = "navy") +
  coord_flip() + 
  theme_bw() +
  labs(title = "Distribution of Pokemon Types", 
       x = "Pokemon Type (type_1)", 
       y = "Count")

```

How many classes of the outcome are there? Are there any Pokémon types with very few Pokémon? If so, which ones?

> There are 18 classes of the outcome, with varying counts for each class. There are Pokemon types with very few Pokemon. These types include: flying (lowest), fairy, and ice.

For this assignment, we'll handle the rarer classes by grouping them, or "lumping them," together into an 'other' category. [Using the `forcats` package](https://forcats.tidyverse.org/), determine how to do this, and **lump all the other levels together except for the top 6 most frequent** (which are Bug, Fire, Grass, Normal, Water, and Psychic).

```{r}
# Convert type_1 column so that there are only 6 class. For rest of classes, lump them into an "other" category
pokemon <- pokemon %>%
  mutate(type_1 = fct_lump(type_1, n = 6))

# Print unique values of type_1 columnn to verify
unique(pokemon$type_1)
```

c.  Convert `type_1`, `legendary`, and `generation` to factors.

```{r}
# Convert type_1, legendary, and generation to factors
pokemon <- pokemon %>%
  mutate(
    type_1 = as.factor(type_1),
    legendary = as.factor(legendary),
    generation = as.factor(generation)
  )

# Verify change
# str(pokemon)
```

d.  Do an initial split of the data; you can choose the percentage for splitting. Stratify on the outcome variable.

```{r}
# Split pokemon data using 3/4 split and stratify by type_1
pokemon_split <- initial_split(pokemon, strata = 'type_1', prop = 0.75)

# Extract training set
pokemon_train <- training(pokemon_split)

# Extract testing set
pokemon_test <- testing(pokemon_split)

```

e.  Fold the training set using *v*-fold cross-validation, with `v = 5`. Stratify on the outcome variable.

```{r}
# Create 5 cross-fold validation sets and stratify by type_1
pokemon_fold <- vfold_cv(pokemon_train, v = 5, strata = type_1)
```

f.  Set up a recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def`:

-   Dummy-code `legendary` and `generation`;
-   Center and scale all predictors.

```{r}
# Create recipe using specification above. Create dummy variables for categorical variable types. Center and scale predictors.
pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def, data = pokemon_train) %>%
  step_dummy(legendary) %>%
  step_dummy(generation) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())

```

### Exercise 2

Create a correlation matrix of the training set, using the `corrplot` package.

```{r}
# Create correlation matrix of training set using numeric values only
pokemon_train %>%
  select(where(is.numeric)) %>%
  cor() %>%
  corrplot()
```

What relationships, if any, do you notice? Do these relationships make sense to you?

> First, I noticed that most predictors are positively correlated. This makes intuitive sense because if your speed value goes up, so will your ability to defend yourself or attack another. In addition, I also noticed that the total variable is highly correlated with all other variables, which also makes sense because it is the sum of all other predictors or overall strength.

### Exercise 3

First, set up a decision tree model and workflow. Tune the `cost_complexity` hyperparameter. Use the same levels we used in the Lab -- that is, `range = c(-3, -1)`. Specify that the metric we want to optimize is `roc_auc`.

```{r}
# Specify a decision tree classification model where the cost_complexity hyperparamter is set to be tuned and set appropriate engine
tree_spec <- decision_tree(cost_complexity = tune()) %>%
  set_engine("rpart") %>% 
  set_mode("classification")

# Set up workflow with decision tree model and recipe specified above
pokemon_wf <- workflow() %>% 
  add_model(tree_spec) %>% 
  add_recipe(pokemon_recipe)

# Create grid of potential cost_complexity values from lab
pokemon_grid <- tibble(cost_complexity = 10 ^ seq(-3, -1, length.out = 10))

# Tune the cost_complexity hyperparamter used cross-fold validation sets specified above. Set roc_auc as the metric to tune by
tune_results <- tune_grid(
  pokemon_wf,
  resamples = pokemon_fold,
  grid = pokemon_grid,
  metrics = metric_set(roc_auc)
)

```

Print an `autoplot()` of the results. What do you observe? Does a single decision tree perform better with a smaller or larger complexity penalty?

```{r}
# Visualize results
autoplot(tune_results) +
  theme_bw()
```

> After observing this plot, I noticed that higher AUC values occur with lower cost-complexity penalties. This iicates that our model performs better when it retains more complexity. Excessive pruning can overly simplify the model, leading to underfitting and a loss of important patterns. Overall, our single decision tree achieves better performance with smaller complexity penalties, as this allows it to effectively capture the patterns in the data.

### Exercise 4

What is the `roc_auc` of your best-performing pruned decision tree on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
# Collect roc_auc values from tuned results and arrange in descending order of mean. Select top value
pruned_metrics <- collect_metrics(tune_results) %>%
  arrange(desc(mean)) %>%
  head(1) %>% 
  select(.config, cost_complexity, .metric, mean)

# Print roc_auc of best-performing pruned decision tree
pruned_metrics %>% 
  gt() %>% 
  tab_header(title = "Best-performing pruned decision tree")
```

> The ROC AUC of my best performing pruned decision tree was `r round(pruned_metrics[4], 3)`.

### Exercise 5

Using `rpart.plot`, fit and visualize your best-performing pruned decision tree with the *training* set.

```{r warning=FALSE, message=FALSE}
# Select the best-performing pruned decision tree
best_complexity <- select_best(tune_results, metric = "roc_auc")

# Apply best-performzing decision tree model on workflow
tree_best <- finalize_workflow(pokemon_wf, best_complexity)

# Fit the best decision tree model on training set
tree_final_fit <- fit(tree_best, data = pokemon_train)

# Visualize results
tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

### Exercise 6

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`. Tune `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.

```{r}
# Specify a random forest forest model where mtry, trees, and min_n are tuned. Set appropriate engine and importance to "impurity"
randForest_pokemon <- rand_forest(mtry = tune(), 
                           trees = tune(), 
                           min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

# Create worfklow with randForest model and add recipe above
pokemon_wf_randForest <- workflow() %>% 
  add_model(randForest_pokemon) %>% 
  add_recipe(pokemon_recipe)
```

> The 'mtry' hyperparameter specifies the number of predictors randomly selected at each split during tree construction. The 'trees' parameter determines the total number of trees in the forest. The 'min_n' hyperparameter defines the minimum number of data points needed to split a node.

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why not. What type of model would `mtry = 8` represent?**

```{r}
# Create a grid with plausible values for each hyperparameter that needs tuning
pokemon_grid <- grid_regular(mtry(range = c(1, 5)), 
                        trees(range = c(100, 300)),
                        min_n(range = c(2, 5)),
                        levels = 8)
```

> If mtry was 8, then that would just be considering every single predictor at each split. This approach negates the randomness introduced by selecting a subset of predictors for each split, which is the advantage of a random forest model. This would results in a single decision tree rather than an ensemble of trees.

### Exercise 7

Specify `roc_auc` as a metric. Tune the model and print an `autoplot()` of the results. What do you observe? What values of the hyperparameters seem to yield the best performance?

```{r}
# Tune the model with the random forest workflow, cross-fold validation sets, and grid of plausible hyperparameters for the best roc_auc value
tune_rf_pokemon <- tune_grid(
  pokemon_wf_randForest,
  resamples = pokemon_fold,
  grid = pokemon_grid,
  metrics = metric_set(roc_auc)
)

# Visualize results
autoplot(tune_rf_pokemon) +
  theme_bw()
```

> From observing the plots, it looks like the mtry hyper parameter, represented by the different colored lines tends to have a higher ROC AUC value the more predictors are randomly sampled at each split. Setting that hyper parameter to 5 predictors seems to be the best option for our model. The next hyper parameter, minimal node size for splitting, looks to be the best for either node size 3 or 5. Both of those have a high AUC value overall. The final hyperparamter is the number of trees, and that stays relatively consistent throughout, but this is dependent on how we are fixing our other hyper paramters. The following hyper parameters may give us the best model predictions: min_n = 3, trees = 275, and mtry = 4.

### Exercise 8

What is the `roc_auc` of your best-performing random forest model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
# Collec roc_auc values for tuned model, arrange by mean, and select top value
randForest_metrics <- collect_metrics(tune_rf_pokemon) %>%
  arrange(desc(mean)) %>%
  head(1)

# Print results
randForest_metrics %>% 
  select(.metric, mtry, trees, min_n, mean) %>% 
  gt() %>% 
  tab_header(title = "Hyperparameters for Best Performing RF Model")
```

> The ROC AUC of my best performing random forest model is `r round(randForest_metrics$mean, 3)`.

### Exercise 9

Create a variable importance plot, using `vip()`, with your best-performing random forest model fit on the *training* set.

```{r}
# Select best performing random forest model from tuning step
best_complexity_rf <- select_best(tune_rf_pokemon)

# Apply workflow to best performing random forest model 
randForest_best <- finalize_workflow(pokemon_wf_randForest, best_complexity_rf)

# Fit best model workflow on training set
randForest_final_fit <- fit(randForest_best, data = pokemon_train)

# Visualize variable importance 
randForest_final_fit %>% extract_fit_parsnip() %>% 
  vip() +
  theme_minimal() +
  ggtitle("Variable Importance of Pokemon Predictors")
```

Which variables were most useful? Which were least useful? Are these results what you expected, or not?

> The variables that were most useful were special attack, speed, and attack. The least useful variables were the generation variables. These results make sense because because special attacks are what makes a Pokemon unique from others so it is expected that it is the most important variable. It also makes sense that generation is not useful because every Pokemon has multiple generations.

### Exercise 10

Finally, set up a boosted tree model and workflow. Use the `xgboost` engine. Tune `trees`. Create a regular grid with 10 levels; let `trees` range from 10 to 2000. Specify `roc_auc` and again print an `autoplot()` of the results.

```{r}
# Specify a boosted tree classification model where trees are tuned. Specify xgboost engine.
boosted_pokemon_model <- boost_tree(trees = tune()) %>%
  set_engine("xgboost") %>% 
  set_mode("classification")

# Set up workflow with boosted tree model and add recipe from above
boosted_pokemon_wf <- workflow() %>% 
  add_model(boosted_pokemon_model) %>% 
  add_recipe(pokemon_recipe)

# Create grid of trees with appropriate range and 10 levels
pokemon_grid_boosted <- grid_regular(trees(range = c(10, 2000)),
                        levels = 10)

# Tune hyper parameters using boosted tree workflow, cross-validation folds, and specified range of trees from previous step. Use roc_auc as metric for tuning
tune_boosted_pokemon <- tune_grid(
  boosted_pokemon_wf,
  resamples = pokemon_fold,
  grid = pokemon_grid_boosted,
  metrics = metric_set(roc_auc)
)

# Visualize results
autoplot(tune_boosted_pokemon) +
  theme_bw() +
  ggtitle("Tree Tuning Step for Boosted Tree Model")
```

What do you observe?

> From this ROC AUC plot, I observe that the highest value for the AUC is when there are around 250 trees. After that peak, there is a gradual descent in the AUC value as the number of trees increases.

What is the `roc_auc` of your best-performing boosted tree model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
# Collect metrics from runed boost tree model, arrange by descending mean, and select top value
boosted_metrics <- collect_metrics(tune_boosted_pokemon) %>%
  arrange(desc(mean)) %>%
  head(1)

# Print results
boosted_metrics %>% 
  select(.metric, trees, mean) %>% 
  gt() %>% 
  tab_header(title = "Hyperparameter for Best Performing Boosted Tree Model")
```

> The ROC AUC value of my best performing boosted tree model is `r round(boosted_metrics$mean, 3)`.

### Exercise 11

Display a table of the three ROC AUC values for your best-performing pruned tree, random forest, and boosted tree models. Which performed best on the folds?

```{r}
# Create dataframe of roc_auc values according to model type
roc_auc_table <- data.frame(
  Model = c("Pruned tree", "Random forest", "Boosted tree"),
  ROC_AUC_Values = c(pruned_metrics$mean, randForest_metrics$mean, boosted_metrics$mean)
)

# Print results
roc_auc_table %>% 
  gt() %>% 
  tab_header(title = "ROC-AUC Values For Each Model")
```

> The model that performed best on the folds was the random forest model with an AUC of `r round(randForest_metrics$mean, 3)`.

Select the best of the three and use `select_best()`, `finalize_workflow()`, and `fit()` to fit it to the *testing* set.

```{r warning=FALSE, message=FALSE}
# Select best-performing model from random forest tuning step
best_complexity_rf <- select_best(tune_rf_pokemon)

# Apply best-performing random forest model to random forest workflow
randForest_best <- finalize_workflow(pokemon_wf_randForest, best_complexity_rf)

# Fit best-performing model to testing data
randForest_final_fit_test <- fit(randForest_best, data = pokemon_test)
```

Print the AUC value of your best-performing model on the testing set. Print the ROC curves. Finally, create and visualize a confusion matrix heat map.

```{r}
# Add predictions to original dataset with type_1 outcome
randForest_test_model <- augment(randForest_final_fit_test, pokemon_test) %>%
  select(type_1, starts_with('.pred'))

# Print roc_auc value for test set
roc_auc(randForest_test_model, truth=type_1, .pred_Bug:.pred_Other) %>% 
  gt()
```

```{r}
# Plot roc_auc curves
roc_curve(randForest_test_model,
          truth = type_1,
          .pred_Bug:.pred_Other) %>% 
  autoplot()
```

```{r}
# Plot confusion matrix
conf_mat(randForest_test_model, 
         truth = type_1,
         .pred_class) %>%
  autoplot(type = 'heatmap')
```

Which classes was your model most accurate at predicting? Which was it worst at?

> My model accurately predicted the type_1 of every Pokemon within the test set. This seems very unlikely. I cannot identify where my code went wrong after reviewing it many times. One potential reason for this highly unlikely outcome is that the type_1 values were changed to have the top 6 most common values and the rest of the values were lumped into "Other". This could have made is easier to identify type_1. I've looked at this script for hours and cannot identify where I made a mistake.
